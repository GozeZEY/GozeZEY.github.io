<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>pytorch学习基础 | Hexo</title><script>var config = </script><script>window.onload = ()=>{};</script><link rel="stylesheet" href="/css/arknights.css"><link rel="stylesheet" href="//unpkg.com/@highlightjs/cdn-assets@11.4.0/styles/atom-one-dark-reasonable.min.css"><style>@font-face {
 font-family: BenderLight;
 src: local('Bender'), url("/font/BenderLight.ttf");
}
@font-face {
 font-family: 'JetBrains Mono';
 src: local('JetBrains Mono'), url('/font/JetBrainsMono-Regular.woff2') format('woff2');
}</style><meta name="generator" content="Hexo 6.0.0"></head><body style="background-image:url(https://ak.hypergryph.com/assets/index/images/ak/pc/bk.jpg);"><div id="cursor-container"><div id="cursor-outer"></div><div id="cursor-effect"></div></div><header><nav><a href="/">Home</a><a href="/archives/">Archives</a></nav></header><main><article><div id="post-bg"><div id="post-title"><h1>pytorch学习基础</h1><div id="post-info"><a herf="_posts/pytorch学习基础.md">View Source File</a><br><span>First Post:<span class="control"><time datetime="2022-03-03T14:26:16.000Z" id="date"> 2022-03-03</time></span></span><br><span>Last Update:<span class="control"><time datetime="2022-03-03T14:30:27.112Z" id="updated"> 2022-03-03</time></span></span></div></div><hr><div id="post-content"><h4 id="1-环境配置"><a href="#1-环境配置" class="headerlink" title="1.环境配置"></a>1.环境配置</h4><p>1.下载anaconda：官网下载；</p>
<p>2.添加镜像，命令如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/<br>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/<br>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/<br>conda config --<span class="hljs-built_in">set</span> show_channel_urls yes<br></code></pre></td></tr></table></figure>

<p>3.创建新环境，然后进入新环境中（-n 后面跟着的是设置的环境名字），命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">conda create -n pytorch<br></code></pre></td></tr></table></figure>

<p>   然后进入环境：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">conda activate pytorch<br></code></pre></td></tr></table></figure>

<p>4.下载pytorch模块，命令如下（CPU版，windows系统，适合集显）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">conda install pytorch torchvision cpuonly<br></code></pre></td></tr></table></figure>

<p>​    此处后面不用加官网命令原版给的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">-c pytorch<br></code></pre></td></tr></table></figure>

<p>其他根据自己python的版本和操作系统，可以去官网查询命令，根据自己的需求，网址：<a target="_blank" rel="noopener" href="https://pytorch.org/">https://pytorch.org/</a></p>
<p>5.新建的环境是没有安装ipykernel的，所以无法注册到Jupyter Notebook中，所以先要准备下环境：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 安装ipykernel</span><br> conda install ipykernel <br><span class="hljs-comment"># 写入环境</span><br> python -m ipykernel install  --name torch --display-name <span class="hljs-string">&quot;Pytorch for Deeplearning&quot;</span><br><span class="hljs-comment"># 切换回基础环境</span><br> activate base<br><span class="hljs-comment"># 创建jupyter notebook配置文件</span><br> jupyter notebook --generate-config<br><span class="hljs-comment"># 这里会显示创建jupyter_notebook_config.py的具体位置</span><br></code></pre></td></tr></table></figure>

<p>6.通过输入jupyter notebook进入到jupyter notebook中，或者直接通过anaconda自带的jupyter notebook快捷方式进入；</p>
<p>注：进入后，kernel选择Pytorch for Deeplearning，即之前自己设置的名字；</p>
<p>7.如果是miniconda，记得在新环境中pip install jupyter；</p>
<hr>
<h4 id="2-pytorch数据加载和工具"><a href="#2-pytorch数据加载和工具" class="headerlink" title="2.pytorch数据加载和工具"></a>2.pytorch数据加载和工具</h4><h5 id="2-1-Dataset类："><a href="#2-1-Dataset类：" class="headerlink" title="2.1 Dataset类："></a>2.1 Dataset类：</h5><p>​        提供一种方式去获取特征值数据和标签值数据（label）；</p>
<p>所有数据集都需要继承该类，__getitem__方法必须重写，__len__方法也可以重写，__init__构造方法初始化；</p>
<p>用<strong>self</strong>指定的变量，相当于你重写的类当中的全局变量，可供后面函数使用；</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyData</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, dir1</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-comment"># 构造函数，初始化数据集</span><br>        <span class="hljs-comment"># 可以继承父类的构造函数</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-keyword">return</span> item, label<br>        <span class="hljs-comment"># 根据idx获取相应某个数据特征值集合以及标签值</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.data)<br>    <span class="hljs-comment"># 返回数据集的长度</span><br>    <br>dataset = MyData(data_path)<br><span class="hljs-comment"># 返回的dataset是一个数据集实例对象，可通过下标调用getitem方法返回索引值对应的数据集的某个对象</span><br>dataset2 = MyData(data_path2)<br>train_dataset = dataset + dataset2<br><span class="hljs-comment"># 可以进行拼接（行拼接）</span><br><span class="hljs-comment"># 注：有下划线表明是内置函数，不用通过 实例对象.函数名 调用，可以直接通过实例对象名和参数调用</span><br></code></pre></td></tr></table></figure>

<hr>
<h5 id="2-2-Tensorboard包："><a href="#2-2-Tensorboard包：" class="headerlink" title="2.2  Tensorboard包："></a>2.2  Tensorboard包：</h5><p>​      用于对于训练模型过程的分析；</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br>writer = SummaryWriter(<span class="hljs-string">&#x27;logs&#x27;</span>)<br><span class="hljs-comment"># log是一个文件夹，将事件文件写入该文件夹，不加文件名，则自动生成runs文件夹用于存放，返回一个实例对象</span><br><br>writer.add_image(<span class="hljs-string">&#x27;tag&#x27;</span>, img_tensor, global_step, dataformats)<br><span class="hljs-comment"># tag:图像标题</span><br><span class="hljs-comment"># img_tensor(torch.Tensor,numpy.array,or strign/blobname):图像的数据；</span><br><span class="hljs-comment"># global_step:出现在同一处图像设置处的图像个数</span><br><span class="hljs-comment"># dataformats=&#x27;HWC&#x27;/&#x27;CHW&#x27;/&#x27;HW&#x27; C：通道数；H：高度；W：宽度；</span><br><br>writer.add_scalar(<span class="hljs-string">&#x27;tag&#x27;</span>,scalar_value, global_step)<br><span class="hljs-comment"># tag（string）:图表标题</span><br><span class="hljs-comment"># scalar_value(folat or string/blobname):图表y轴</span><br><span class="hljs-comment"># global_step(int): 图表x轴</span><br><span class="hljs-comment"># 图表对于点会自动拟合，设置tag相同的会放在一张图表里</span><br><span class="hljs-comment"># 用命令行打开事件文件：tensorboard --logdir=logs --post==6007 指定文件夹和显示端口</span><br>writer.close()<br></code></pre></td></tr></table></figure>

<hr>
<h5 id="2-3-Transforms工具箱："><a href="#2-3-Transforms工具箱：" class="headerlink" title="2.3 Transforms工具箱："></a>2.3 Transforms工具箱：</h5><p>​       用于对图像进行类型转换：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><br><span class="hljs-comment"># python当中的用法 -&gt; Tentor数据类型</span><br>img = Image.<span class="hljs-built_in">open</span>(img_path)<br>tensor_trans = transforms.ToTensor()<br><span class="hljs-comment"># 从transform中创建工具</span><br>tentor_img = tensor_trans(img)<br><span class="hljs-comment"># ToTensor可以将PIL.image和numpy.ndarray转变成tensor类型: PIL/numpy.array -&gt; tentor</span><br><span class="hljs-comment"># 为什么要转变和使用tensor类型？</span><br><span class="hljs-comment"># tensor数据类型包含了神经网络理论所需要的一些基础类型参数</span><br><br><span class="hljs-comment"># 常见的transforms</span><br><span class="hljs-comment"># 1.ToTensor:将PIL.image或者numpy.ndarray类型转为tensor类型</span><br><span class="hljs-comment"># 上述</span><br><br><span class="hljs-comment"># 2.Normalize：对tensor类型数据进行归一化: tentor -&gt; 归一化 -&gt; tentor</span><br><span class="hljs-comment"># 输入平均值，标准差</span><br>trans_nor = transforms.Normalize([mean...], [std...])<br>img_nor = trans_nor(tentor_img)<br><br><span class="hljs-comment"># 3.Resize：将PIL.image类型按照设定的size从新输出:PIL -&gt; resize -&gt; img.Resize PIL</span><br>trans_resize = transforms.Resize((H, W))<br>img_resize = trans_resize(img)<br><br><span class="hljs-comment"># 4.Compose -resize -2</span><br><span class="hljs-comment"># 作用，将transforms工具整合成列表，依次执行</span><br><span class="hljs-comment"># 创建工具时，输入均为transforms类型实例对象，第二个transforms的输入类型要和第一个transforms的输出类型相匹配</span><br><span class="hljs-comment">#举例  PIL -&gt; PIL -&gt; tensor </span><br>trans_compose = transforms.Compose([transforms, transforms])<br>img_compose = trans_compose(img)<br><br><span class="hljs-comment"># 5.RandomCrop 随机切片</span><br><span class="hljs-comment"># PIL -&gt; PIL</span><br>trans_rand = transforms.RandomCrop((<span class="hljs-number">500</span>,<span class="hljs-number">1000</span>))<br>img_rand = trans_rand(img)<br></code></pre></td></tr></table></figure>

<p>​    注：使用transforms工具箱关注输入和输出类型以及相应的参数，多关注官方文档；</p>
<hr>
<h5 id="2-4-torchvision数据集："><a href="#2-4-torchvision数据集：" class="headerlink" title="2.4 torchvision数据集："></a>2.4 torchvision数据集：</h5><p>​     torchvision中自带了很多数据集以及一些常规的模型，可以自行通过命令下载，如果下载太慢，可以根据网址通过迅雷的工具下载；</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torchvision<br>train_set = torchvision.datasets.CIFAR10(root=<span class="hljs-string">&quot;./data&quot;</span>, train=<span class="hljs-literal">True</span>,transform=torchvision.transforms.ToTensor(),  download=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># root是数据集的存储路径目录；</span><br><span class="hljs-comment"># train为True则生成训练集，为false则生成测试集</span><br><span class="hljs-comment"># transform设置了数据集中数据的转换方法</span><br><span class="hljs-comment"># download=True 表示是否从包里下载</span><br><span class="hljs-comment"># 返回值是一个dataset数据集类型的实例对象</span><br>img, target =train_set[<span class="hljs-number">0</span>]<br><span class="hljs-comment"># target为数字，对应了classes中对于索引值的值</span><br><span class="hljs-built_in">print</span>(train_set.classes[target])<br></code></pre></td></tr></table></figure>

<hr>
<h5 id="2-5-Dataloader类："><a href="#2-5-Dataloader类：" class="headerlink" title="2.5 Dataloader类："></a>2.5 Dataloader类：</h5><p>为后面的神经网络提供不同的数据形式；</p>
<p>作用：用于将数据集以不同的样式加载到神经网络当中；</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br>dataloader_set = DataLoader(dataset=data_set, batch_size=<span class="hljs-number">4</span>, shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">0</span>,drop_last=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># dataset: dataset数据集类型的对象；</span><br><span class="hljs-comment"># batch_size:每次取几个样本，即取几行；</span><br><span class="hljs-comment"># shuffle:True则为打乱顺序,随机抽取样本，false，顺序和上次一样</span><br><span class="hljs-comment"># num_worker:几个进程，0为就一个主进程</span><br><span class="hljs-comment"># drop_last:取样多余的几个是否舍弃</span><br><span class="hljs-comment"># 返回值也是一个数据集类型</span><br>img, target = dataloader_set[<span class="hljs-number">0</span>]<br></code></pre></td></tr></table></figure>

<hr>
<h4 id="3-神经网络"><a href="#3-神经网络" class="headerlink" title="3.神经网络"></a>3.神经网络</h4><h5 id="3-1-神经网络的基本搭建"><a href="#3-1-神经网络的基本搭建" class="headerlink" title="3.1 神经网络的基本搭建"></a>3.1 神经网络的基本搭建</h5><p>module类:所有神经网络的基本骨架，所有神经网络都要继承它；</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(Model, self).__init__()<br>        <span class="hljs-comment"># 调用父类的构造函数</span><br>        self.conv1 = nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>)<br>        self.conv2 = nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = F.relu(self.conv1(x))<br>        <span class="hljs-keyword">return</span> F.relu(self.conv2(x))<br>        <span class="hljs-comment"># 前向传播，对输入做处理并输出；</span><br>        <span class="hljs-comment"># 输入x -&gt; 卷积 -&gt; 非线性处理 -&gt; 卷积 -&gt; 非线性处理</span><br>        <span class="hljs-comment"># forward函数必须进行重写</span><br><br><br>model = Model()<br>x = torch.tensor(<span class="hljs-number">1.0</span>)<br>output = model(x)<br></code></pre></td></tr></table></figure>

<p>conv2d类：卷积操作；</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-built_in">input</span> = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>]<br>                      [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]<br>                      [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]<br>                      [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>]<br>                      [<span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>]<br>                      [<span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>]])<br><br>kernel = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]<br>                       [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]<br>                       [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>]])<br><br>torch.reshape(<span class="hljs-built_in">input</span>, (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>))<br><span class="hljs-comment"># 重新设置尺寸：batch_size channel H W</span><br><span class="hljs-comment"># 如何有不晓得尺寸的输入-1，会根据其他的参数自动调整</span><br>torch.reshape(<span class="hljs-built_in">input</span>, (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>))<br>output = F.conv2d(<span class="hljs-built_in">input</span>, kernel, stride=<span class="hljs-number">1</span>)<br><span class="hljs-comment"># input 输入图像，要求有batch channel H W 四个尺寸</span><br><span class="hljs-comment"># kernel 卷积核，要求同上</span><br><span class="hljs-comment"># stride 移动步长，可以为1维，1维平移，2维平移和上下都设置</span><br><span class="hljs-comment"># padding 进行填充，默认为0，同样可以一维或者二维，一维只在input的左右两边插入，二维左右上下都插入</span><br><span class="hljs-comment"># 输出为tensor类型的矩阵</span><br></code></pre></td></tr></table></figure>

<hr>
<h5 id="3-2-卷积层"><a href="#3-2-卷积层" class="headerlink" title="3.2 卷积层"></a>3.2 卷积层</h5><p>conv1d：一维卷积；</p>
<p>conv2d：二维卷积；（常用）</p>
<p>conv3d：三维卷积；</p>
<p>举例：    Conv2d：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><br><span class="hljs-comment"># in_channel 输入的图像的特征数，彩色图像一般为3</span><br><span class="hljs-comment"># out_channel 输出的层数，会依次对卷积核的层数进行改变</span><br><span class="hljs-comment"># batch_size 一层几个图像</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=, stride=, padding=)<br>        <span class="hljs-comment"># 生成相应的实例化对象</span><br>        <span class="hljs-comment"># in_channels(int) 输入图像的特征数；</span><br>        <span class="hljs-comment"># out_channels(int)自己设置的输出图像的特征数；</span><br>        <span class="hljs-comment"># kernel_size(int/tuple) 卷积核和尺寸，可以用整数或者元组设置</span><br>        <span class="hljs-comment"># stride和padding 用法和F.conv2d一样</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.conv(x)<br>        <span class="hljs-keyword">return</span> x<br><br><br><span class="hljs-comment"># 一般init中设置网络各层，forward调用使用</span><br><br>model = Model()<br>output = model(dataloader_set)<br><span class="hljs-comment"># 和F.conv2d一样，输入x必须有batch_size channel H W四个尺寸</span><br></code></pre></td></tr></table></figure>

<hr>
<h5 id="3-3-池化层"><a href="#3-3-池化层" class="headerlink" title="3.3 池化层"></a>3.3 池化层</h5><p>最大池化的使用：</p>
<p>​    原理：对于池化核映射到输入的部分相应求积，和卷积核不同的是这次取最大值，且无步长，采用的类似于分割，目的是        大大减少数据量，加快训练速度；</p>
<p>举例：MaxPool2d：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-built_in">input</span> = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>]<br>                      [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]<br>                      [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]<br>                      [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>]<br>                      [<span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>]<br>                      [<span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>]], dtype=torch.float32)<br><span class="hljs-comment"># torch.tnesor可以指定类型，核array核framedat有点像</span><br><span class="hljs-built_in">input</span> = torch.reshape(<span class="hljs-built_in">input</span>, (-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>))<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.pool = nn.MaxPool2d(kernel_size, ceil_mode)<br>        <span class="hljs-comment"># kernel_size 池化核的尺寸</span><br>        <span class="hljs-comment"># ceil_mode 对于多余不满足大小的部分true则保留，false则舍弃</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.pool(x)<br>        <span class="hljs-keyword">return</span> x<br>model = Model()<br>output = model(<span class="hljs-built_in">input</span>)<br></code></pre></td></tr></table></figure>

<hr>
<h5 id="3-4-非线性激活"><a href="#3-4-非线性激活" class="headerlink" title="3.4 非线性激活"></a>3.4 非线性激活</h5><p>（padding层省略）</p>
<p>作用：做输入特征做非线性变换，以适应各种非线性曲线</p>
<p>EeLU类和Sigmoid类（简单）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> ReLU, Sigmoid<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.ReLU = ReLU(<span class="hljs-built_in">input</span>,inplace)<br>        <span class="hljs-comment"># input 四个尺寸的tensor类型变量</span><br>        <span class="hljs-comment"># inplace 对输入变量是否用output输出变量进行替换</span><br>        self.sigmod = Sigmoid(<span class="hljs-built_in">input</span>)<br>        <span class="hljs-comment"># input 四个尺寸的tensor类型变量</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.ReLU(x)<br>        x= self.sigmod(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>

<p>注：其他非线性变换无非只是处理公式的不同，可自主取官网查询；</p>
<hr>
<h5 id="3-5-正则化层等层"><a href="#3-5-正则化层等层" class="headerlink" title="3.5 正则化层等层"></a>3.5 正则化层等层</h5><p>正则化层：类比于传统机器学习的标准化，用于减小偏差和方差；</p>
<p>Recurrent 层：用的不多；</p>
<p>Transformer层：用的不多；</p>
<p>线性层：用于生产分类；</p>
<p>Dropout层：用的不多，主要为了防止过拟合；</p>
<p>Sparse层：用的不多，主要用于自然语言处理；</p>
<hr>
<h5 id="3-6-搭建示例（1）"><a href="#3-6-搭建示例（1）" class="headerlink" title="3.6 搭建示例（1）"></a>3.6 搭建示例（1）</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.utils.tensorboard<br><span class="hljs-keyword">import</span> torchvision<br><br><span class="hljs-comment"># dataset_train = torchvision.datasets.CIFAR10(&#x27;./data&#x27;, transform=torchvision.transforms.ToTensor(), train=True,</span><br><span class="hljs-comment">#                                              download=True)</span><br><span class="hljs-comment"># dataset_train = torchvision.datasets.CIFAR10(&#x27;./data&#x27;, transform=torchvision.transforms.ToTensor(), train=False,</span><br><span class="hljs-comment">#                                              download=True)</span><br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.conv = nn.Conv2d(in_channels=<span class="hljs-number">3</span>, out_channels=<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>)<br>        self.Maxpool2d = nn.MaxPool2d(<span class="hljs-number">2</span>)<br>        self.conv2 = nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>)<br>        self.Maxpool2d2 = nn.MaxPool2d(<span class="hljs-number">2</span>)<br>        self.conv3 = nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>)<br>        self.Maxpool2d3 = nn.MaxPool2d(<span class="hljs-number">2</span>)<br>        self.flattn = nn.Flatten()<br>        self.linear1 = nn.Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">64</span>)<br>        self.linear2 = nn.Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>)<br>        self.model1 = nn.Sequential(<br>            nn.Conv2d(in_channels=<span class="hljs-number">3</span>, out_channels=<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Flatten(),<br>            nn.Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">64</span>),<br>            nn.Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.model1(x)<br>        <span class="hljs-keyword">return</span> x<br><br><br>model = Model()<br><span class="hljs-built_in">input</span> = torch.ones((<span class="hljs-number">64</span>, <span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>))<br>output = model(<span class="hljs-built_in">input</span>)<br><span class="hljs-built_in">print</span>(output.shape)<br>writer = SummaryWriter(<span class="hljs-string">&quot;./logs&quot;</span>)<br>writer.add_graph(model, <span class="hljs-built_in">input</span>)<br>writer.close()<br></code></pre></td></tr></table></figure>

<hr>
<h5 id="3-7-损失函数和反向传播"><a href="#3-7-损失函数和反向传播" class="headerlink" title="3.7 损失函数和反向传播"></a>3.7 损失函数和反向传播</h5><p>Loss Functions作用：计算实际输出（预测值）和目标（目标值）之间的差距，且维我们更新输出提供了一定的依据（反向传播），得出损失函数之后，调用返回值的backward得到反向·传播的值；</p>
<hr>
<h5 id="3-8-优化器"><a href="#3-8-优化器" class="headerlink" title="3.8 优化器"></a>3.8 优化器</h5><p>作用：用于模型参数调优，会通过梯度下降法，调整神经网络中的参数从而使得loss下降</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python">model = Fake1DAttention(in_features, out_features)<br><span class="hljs-comment"># 模型</span><br> loss_fn = nn.CrossEntropyLoss()<br><span class="hljs-comment"># 损失函数用交叉熵</span><br> optimizer = optim.Adagrad(model.parameters(), lr=lr)<br><span class="hljs-comment"># 生成优化器，参数第一个为模型的参数，第二个为训练的速率</span><br><br><br><br>optimizer.zero_grad()<br><span class="hljs-comment"># 梯度归零</span><br>loss.backward()<br><span class="hljs-comment"># 反向传播</span><br>optimizer.step()<br><span class="hljs-comment"># 参数优化更新</span><br>（此三行代码为内层循环中代码，外层循环是训练次数）<br></code></pre></td></tr></table></figure>

<hr>
<h5 id="3-9-完整的模型训练套路"><a href="#3-9-完整的模型训练套路" class="headerlink" title="3.9 完整的模型训练套路"></a>3.9 完整的模型训练套路</h5><h6 id="网络模型的保存与加载："><a href="#网络模型的保存与加载：" class="headerlink" title="网络模型的保存与加载："></a>网络模型的保存与加载：</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><br>model = torchvision.models.vgg16(pretrained=<span class="hljs-literal">False</span>)<br><span class="hljs-comment"># 方案1 模型结构和模型参数</span><br>torch.save(model, <span class="hljs-string">&#x27;model.pth&#x27;</span>)<br>model2 = torch.load(<span class="hljs-string">&#x27;model.pth&#x27;</span>)<br><br><span class="hljs-comment"># 方案2 模型参数（官方推荐）</span><br>torch.save(model.state_dict(), <span class="hljs-string">&#x27;model2.pth&#x27;</span>)  <span class="hljs-comment"># 以字典形式保存参数</span><br>model2 = torch.load(<span class="hljs-string">&#x27;model2.pth&#x27;</span>)  <span class="hljs-comment"># 打印也是字典形式的参数</span><br><span class="hljs-comment"># 将参数加载进原始模型中</span><br>model3 = torchvision.models.vgg16(pretrained=<span class="hljs-literal">False</span>)<br>model3.load_state_dict(model2)<br><br><span class="hljs-comment"># 方案1的不足</span><br><span class="hljs-comment"># 方案1，自定义的模型，必须引入源码才能加载到，直接通过pth文件名找不到</span><br></code></pre></td></tr></table></figure>

<p>模型训练套路：</p>
<h6 id="train文件："><a href="#train文件：" class="headerlink" title="train文件："></a>train文件：</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset, DataLoader<br><span class="hljs-keyword">from</span> model <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">from</span> colorama <span class="hljs-keyword">import</span> Fore<br><span class="hljs-keyword">from</span> metric <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SeedDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br><span class="hljs-comment"># 重写Dataset函数，init函数，getitem和len函数，init和getitem必须重写</span><br><span class="hljs-comment"># init函数用super继承父类的初始化构造函数，该函数用于获取数据，形成数据集（特征工程可以用sklearn测试）</span><br><span class="hljs-comment"># len函数返回数据集的大小</span><br><span class="hljs-comment"># getitem函数，重要，用于根据索引返回相应的数据记录</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, annotations_file</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.data: pd.DataFrame = pd.read_csv(annotations_file)<br>        self.data: pd.DataFrame = self.data[self.data[<span class="hljs-string">&#x27;label&#x27;</span>].notna()]<br>        self.Y = self.data[<span class="hljs-string">&#x27;label&#x27;</span>]<br>        self.X = self.data.drop(columns=[<span class="hljs-string">&#x27;id&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>]).fillna(value=-<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.data)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-keyword">return</span> torch.as_tensor(self.X.iloc[idx].values).<span class="hljs-built_in">type</span>(torch.FloatTensor), torch.as_tensor(self.Y.iloc[idx]).<span class="hljs-built_in">type</span>(<br>            torch.LongTensor)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">dataloader, model, loss_fn, optimizer, device, positive_weight</span>):<br>    model.train()<br>    <span class="hljs-comment"># 设置训练状态，使得BN层和drop层生效</span><br>    Y = []<br>    <span class="hljs-keyword">for</span> batch, (X, y) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(dataloader):<br>        <span class="hljs-comment"># enumrate函数，将其变成列表形式，batch为第几个，X为特征，y为标签</span><br>        X, y = X.to(device), y.to(device)<br>        <span class="hljs-comment"># 使用cpu进行训练</span><br>        logit = model(X)<br>        <span class="hljs-comment"># 开始训练</span><br>        positive_index = y == <span class="hljs-number">1</span><br>        <span class="hljs-comment"># 取出这一次训练中（此处为64个）中标签为1的记录条数，方便后续对损失函数进行权重分割</span><br>        loss = loss_fn(logit, y)<br>        <span class="hljs-comment"># 计算该次的损失函数</span><br>        loss = (positive_weight * loss_fn(logit[positive_index], y[positive_index]) + loss_fn(logit[~positive_index], y[<br>            ~positive_index])) / <span class="hljs-built_in">len</span>(X)<br>        <span class="hljs-comment"># 加权之后的损失函数</span><br>        optimizer.zero_grad()<br>        <span class="hljs-comment"># 三部曲，1优化器归零</span><br>        loss.backward()<br>        <span class="hljs-comment"># 2反向传播</span><br>        optimizer.step()<br>        <span class="hljs-comment"># 3优化器优化</span><br>        <span class="hljs-keyword">if</span> batch % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>            loss = loss.item()<br>            <span class="hljs-comment"># 因为loss为tentor类型，item取出值</span><br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;Fore.GREEN + <span class="hljs-string">&#x27;[train]===&gt;&#x27;</span>&#125;</span> loss: <span class="hljs-subst">&#123;loss&#125;</span> <span class="hljs-subst">&#123;<span class="hljs-string">&#x27;&#x27;</span> + Fore.RESET&#125;</span>&quot;</span>)<br>            <span class="hljs-comment"># 为防止计算过多，每一百次输出一次loss</span><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">valid</span>(<span class="hljs-params">dataloader, model, loss_fn, device</span>):<br>    model.<span class="hljs-built_in">eval</span>()<br>    <span class="hljs-comment"># 设置为验证模式，使得BN层，drop层失效</span><br>    num_dataset = <span class="hljs-built_in">len</span>(dataloader.dataset)<br>    loss = <span class="hljs-number">0</span><br>    <span class="hljs-comment"># 初始化loss,用于统计每轮验证集总loss</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-comment"># 套路，使得减少使用运存</span><br>        pred, Y = [], []<br>        <span class="hljs-keyword">for</span> batch, (X, y) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(dataloader):<br>            X, y = X.to(device), y.to(device)<br>            <span class="hljs-comment"># enumerate,.to()函数作用同上</span><br>            logit = model(X)<br>            loss += loss_fn(logit, y).item()<br>            <span class="hljs-comment"># 累加loss</span><br>            pred.append(logit.argmax(<span class="hljs-number">1</span>))<br>            <span class="hljs-comment"># argmax(1) 获得每行最大值的索引（即可能性最大的那一类，针对分类问题）</span><br>            Y.append(y)<br>            <span class="hljs-comment"># 每一次训练的都添加进去</span><br>        loss /= num_dataset<br>        <span class="hljs-comment"># 算平均loss</span><br>        pred = torch.cat(pred)<br>        Y = torch.cat(Y)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;Fore.CYAN + <span class="hljs-string">&#x27;[valid]===&gt;&#x27;</span>&#125;</span> &quot;</span> <br>              <span class="hljs-string">f&quot;loss: <span class="hljs-subst">&#123;loss&#125;</span>  acc: <span class="hljs-subst">&#123;<span class="hljs-number">100</span> * Accuracy(pred, Y)&#125;</span>%  precision: <span class="hljs-subst">&#123;Precision(pred, Y)&#125;</span>  recall: <span class="hljs-subst">&#123;Recall(pred, Y)&#125;</span>   fscore: <span class="hljs-subst">&#123;Fscore(pred, Y)&#125;</span>&quot;</span> \<br>              <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;<span class="hljs-string">&#x27;&#x27;</span> + Fore.RESET&#125;</span>&quot;</span>)<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br><br>    torch.manual_seed(<span class="hljs-number">777</span>)<br>    <span class="hljs-comment"># 设置随机种子，使得每次Dataloader抽样都一样</span><br>    device = torch.device(<span class="hljs-string">&#x27;cpu&#x27;</span>)<br>    <span class="hljs-comment"># 使用cpu进行训练，后续用.to(device)的函数均为该作用</span><br><br>    batch_size, in_features, out_features = <span class="hljs-number">30</span>, <span class="hljs-number">28</span>, <span class="hljs-number">2</span><br>    lr, positive_weight = <span class="hljs-number">1e-3</span>, <span class="hljs-number">2.33</span><br>    <span class="hljs-comment"># 设定优化器的速率，以及之后算交叉熵的时候的权重</span><br>    epochs = <span class="hljs-number">28</span><br>    <span class="hljs-comment"># 设定训练轮数</span><br>    model = Fake1DAttention(in_features, out_features)<br>    <span class="hljs-comment"># 获得模型对象</span><br>    loss_fn = nn.CrossEntropyLoss()<br>    <span class="hljs-comment"># 获得损失函数对象（使用交叉熵，内置了softmax函数）</span><br>    optimizer = optim.Adagrad(model.parameters(), lr=lr)<br>    <span class="hljs-comment"># 获得优化器对象</span><br>    train_dataset = SeedDataset(<span class="hljs-string">&quot;./data/v1/train.csv&quot;</span>)<br>    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=<span class="hljs-literal">True</span>)<br>    <span class="hljs-comment">#初始化数据集以及打乱顺序随机抽样（类似于分割，每次取出batch_size条记录作为一份）作为训练集使用</span><br>    valid_dataset = SeedDataset(<span class="hljs-string">&quot;./data/v1/valid.csv&quot;</span>)<br>    valid_dataloader = DataLoader(valid_dataset, batch_size=<span class="hljs-number">1</span>, shuffle=<span class="hljs-literal">False</span>)<br>    <span class="hljs-comment"># 初始化自己设置的测试集，效果同上，不过这个是非随机抽样</span><br>    <br>    <span class="hljs-comment">#训练开始</span><br>    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;Fore.GREEN + <span class="hljs-string">&#x27;===&gt;&#x27;</span>&#125;</span> Epoch <span class="hljs-subst">&#123;t + <span class="hljs-number">1</span>&#125;</span> <span class="hljs-subst">&#123;<span class="hljs-string">&#x27;&#x27;</span> + Fore.RESET&#125;</span>\n&quot;</span> \<br>              <span class="hljs-string">&quot;---------------------------------------&quot;</span>)<br>        train(train_dataloader, model, loss_fn, optimizer, device, positive_weight)<br>        valid(valid_dataloader, model, loss_fn, device)<br>        torch.save(model.state_dict(), <span class="hljs-string">f&quot;./checkpoints/<span class="hljs-subst">&#123;t&#125;</span>_epoc.pt&quot;</span>)<br>        <span class="hljs-comment">#保存模型，使用官方推荐的，将参数进行保存</span><br></code></pre></td></tr></table></figure>

<h6 id="test文件："><a href="#test文件：" class="headerlink" title="test文件："></a>test文件：</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> model <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">import</span> argparse<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset, DataLoader<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">parse_args</span>():<br>    parser = argparse.ArgumentParser()<br>    parser.add_argument(<span class="hljs-string">&#x27;--model&#x27;</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&quot;path to model&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=<span class="hljs-string">&quot;./checkpoints/7_epoc.pt&quot;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;-i&#x27;</span>, <span class="hljs-string">&#x27;--input&#x27;</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&quot;path to input files&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=<span class="hljs-string">&quot;./data/v1/test_a.csv&quot;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;-o&#x27;</span>, <span class="hljs-string">&#x27;--output&#x27;</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&quot;path to output files&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=<span class="hljs-string">&quot;output_a.txt&quot;</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--input-features&#x27;</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&quot;input dimension for model&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">28</span>)<br>    parser.add_argument(<span class="hljs-string">&#x27;--output-features&#x27;</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&quot;output dimension for model&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">2</span>)<br><br>    <span class="hljs-keyword">return</span> parser.parse_args()<br><span class="hljs-comment"># 将需要使用的参数归在一起，构造一个初始化后的结构体</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SeedDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br><span class="hljs-comment"># 重写Dataset函数，讲解如train所见</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, annotations_file</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.data: pd.DataFrame = pd.read_csv(annotations_file)<br>        self.X = self.data.drop(columns=[<span class="hljs-string">&#x27;id&#x27;</span>]).fillna(value=-<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.data)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-keyword">return</span> torch.as_tensor(self.X.iloc[idx].values).<span class="hljs-built_in">type</span>(torch.FloatTensor)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    args = parse_args()<br>    <span class="hljs-comment"># 获得parse对象</span><br>    model = Fake1DAttention(args.input_features, args.output_features)<br>    <span class="hljs-comment"># 开始用模型对验证集进行验证</span><br>    model.load_state_dict(torch.load(args.model))<br>    <span class="hljs-comment"># 加载模型参数</span><br>    model.<span class="hljs-built_in">eval</span>()<br>    <span class="hljs-comment"># 设置验证状态</span><br>    test_dataset = SeedDataset(args.<span class="hljs-built_in">input</span>)<br>    test_dataloader = DataLoader(test_dataset, batch_size=<span class="hljs-number">1</span>, shuffle=<span class="hljs-literal">False</span>)<br>    <span class="hljs-comment"># 初始化和分割</span><br>    outputs = []<br>    <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> test_dataloader:<br>        logit = model(x)<br>        outputs.append(<span class="hljs-built_in">str</span>(logit.argmax(<span class="hljs-number">1</span>).item()))<br>        <span class="hljs-comment"># 取出每次（每条记录）最后可能的类别</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(args.output, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        f.write(<span class="hljs-string">&#x27;\n&#x27;</span>.join(outputs))<br>        <span class="hljs-comment"># 以换行作为分割符写入args.output文件中</span><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    main()<br></code></pre></td></tr></table></figure>

<h6 id="model文件（即模型）："><a href="#model文件（即模型）：" class="headerlink" title="model文件（即模型）："></a>model文件（即模型）：</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Fake1DAttention</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_features, out_features</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Fake 1D attention model as reference.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            in_features (int): input size for feature dimension</span><br><span class="hljs-string">            out_features (int): output size for feature dimension</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>().__init__()<br><br>        self.attn1 = nn.parameter.Parameter(torch.randn(in_features), requires_grad=<span class="hljs-literal">True</span>)<br>        <span class="hljs-comment"># 将in_features给随机值组成的变量，变为可以进行训练使用的参数变量（同时随机生成28个值）</span><br>        self.fc1 = nn.Linear(in_features, in_features)<br>        self.activation1 = nn.ReLU()<br>        self.bn1 = nn.BatchNorm1d(in_features)<br><br>        self.fc2 = nn.Linear(in_features, in_features)<br>        self.activation2 = nn.ReLU()<br>        self.bn2 = nn.BatchNorm1d(in_features)<br><br>        self.fc3 = nn.Linear(in_features, out_features)<br>        self.softmax = nn.Softmax(<span class="hljs-number">1</span>)<br><br>        self.model = nn.Sequential(<br>            self.fc1, self.activation1,  <span class="hljs-comment"># self.bn1,</span><br>            self.fc2, self.activation2,  <span class="hljs-comment"># self.bn2,</span><br>            self.fc3, self.softmax<br>        )<br><span class="hljs-comment"># 三层线性，两层非线性，一层归一化</span><br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> self.model(self.attn1 * x)<br>    <span class="hljs-comment"># 先进行随机化后再开始训练</span><br></code></pre></td></tr></table></figure>
<div id="paginator"></div></div><div id="post-footer"></div></div><div id="bottom-btn"><a id="to-top" onClick="index.scrolltop();" title="to top">∧</a></div></article><div class="aside-box"><aside><div id="aside-top"><div id="about"><a href="/" id="logo"><img src="https://ak.hypergryph.com/assets/index/images/ak/pc/faction/1.png" alt="Logo"></a><h1 id="Dr"><a href="/">John Doe</a></h1><div id="description"><p></p></div><!--if page.published === undefined--><section id="total"><a id="total-archives" href="/archives"><span class="total-title">Archives Total:</span><span class="total-number">1</span></a><div id="total-tags"><span class="total-title">Tags:</span><span class="total-number">1</span></div><div id="total-categories"><span class="total-title">Categories:</span><span class="total-number">0</span></div></section></div><div id="aside-block"><h1>INDEX</h1><div id="toc-div"><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE"><span class="toc-number">1.</span> <span class="toc-text">1.环境配置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-pytorch%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%92%8C%E5%B7%A5%E5%85%B7"><span class="toc-number">2.</span> <span class="toc-text">2.pytorch数据加载和工具</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#2-1-Dataset%E7%B1%BB%EF%BC%9A"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 Dataset类：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-2-Tensorboard%E5%8C%85%EF%BC%9A"><span class="toc-number">2.2.</span> <span class="toc-text">2.2  Tensorboard包：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-3-Transforms%E5%B7%A5%E5%85%B7%E7%AE%B1%EF%BC%9A"><span class="toc-number">2.3.</span> <span class="toc-text">2.3 Transforms工具箱：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-4-torchvision%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%9A"><span class="toc-number">2.4.</span> <span class="toc-text">2.4 torchvision数据集：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-5-Dataloader%E7%B1%BB%EF%BC%9A"><span class="toc-number">2.5.</span> <span class="toc-text">2.5 Dataloader类：</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">3.</span> <span class="toc-text">3.神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#3-1-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%90%AD%E5%BB%BA"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 神经网络的基本搭建</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-2-%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 卷积层</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-3-%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="toc-number">3.3.</span> <span class="toc-text">3.3 池化层</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-4-%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%BF%80%E6%B4%BB"><span class="toc-number">3.4.</span> <span class="toc-text">3.4 非线性激活</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-5-%E6%AD%A3%E5%88%99%E5%8C%96%E5%B1%82%E7%AD%89%E5%B1%82"><span class="toc-number">3.5.</span> <span class="toc-text">3.5 正则化层等层</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-6-%E6%90%AD%E5%BB%BA%E7%A4%BA%E4%BE%8B%EF%BC%881%EF%BC%89"><span class="toc-number">3.6.</span> <span class="toc-text">3.6 搭建示例（1）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-7-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">3.7.</span> <span class="toc-text">3.7 损失函数和反向传播</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-8-%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">3.8.</span> <span class="toc-text">3.8 优化器</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-9-%E5%AE%8C%E6%95%B4%E7%9A%84%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%A5%97%E8%B7%AF"><span class="toc-number">3.9.</span> <span class="toc-text">3.9 完整的模型训练套路</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD%EF%BC%9A"><span class="toc-number">3.9.1.</span> <span class="toc-text">网络模型的保存与加载：</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#train%E6%96%87%E4%BB%B6%EF%BC%9A"><span class="toc-number">3.9.2.</span> <span class="toc-text">train文件：</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#test%E6%96%87%E4%BB%B6%EF%BC%9A"><span class="toc-number">3.9.3.</span> <span class="toc-text">test文件：</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#model%E6%96%87%E4%BB%B6%EF%BC%88%E5%8D%B3%E6%A8%A1%E5%9E%8B%EF%BC%89%EF%BC%9A"><span class="toc-number">3.9.4.</span> <span class="toc-text">model文件（即模型）：</span></a></li></ol></li></ol></li></ol></div></div></div><footer></footer></aside></div></main><canvas id="canvas-dust"></canvas><script src="/js/arknights.js"></script><script src="//unpkg.com/@highlightjs/cdn-assets@11.4.0/highlight.min.js"></script></body></html>